configfile: "config/config.yaml"

import os
import glob
import csv
import datetime
import yaml

# =============================================================================
# Manifest-driven configuration
# =============================================================================

MANIFEST_CSV = config["batch"]["manifest_csv"]
TARGET_BATCH_ID = config["batch"]["batch_id"]


def load_manifest(manifest_csv, batch_id):
    """
    Load manifest rows for a given batch_id and build a per-sample dictionary.

    Key = sample_submitter_id
    Value = dict with metadata that we may use later.
    """
    samples = {}

    # manifest is a CSV (comma-separated)
    with open(manifest_csv, newline="") as f:
        reader = csv.DictReader(f, delimiter=",")
        for row in reader:
            if row.get("batch_id", "") != batch_id:
                continue

            sample_id = row.get("sample_submitter_id", "").strip()
            if not sample_id:
                continue

            lanes_str = row.get("lanes", "")
            lanes = [x.strip() for x in lanes_str.split(";") if x.strip()]

            samples[sample_id] = {
                "project_id":          row.get("project_id", ""),
                "batch_id":            row.get("batch_id", ""),
                "case_id":             row.get("case_submitter_id", ""),
                "sample_id":           sample_id,
                "tumor_descriptor":    row.get("tumor_descriptor", ""),
                "analyte_type":        row.get("analyte_type", ""),
                "fastq_dir":           row.get("fastq_local_dir", ""),
                "lanes":               lanes,
                "i7_index":            row.get("i7_index", ""),
                "i5_index":            row.get("i5_index", ""),
                "library_layout":      row.get("library_layout", ""),
                "specimen_type":       row.get("specimen_type", ""),
                "tissue_preservation": row.get("tissue_preservation_method", ""),
                # optional fields for MultiQC header
                "flowcell_id":         row.get("flowcell_id", row.get("flowcell", "")),
                "run_date":            row.get("run_date", row.get("sequencing_run_date", "")),
            }

    if not samples:
        raise ValueError(
            f"No samples found in manifest {manifest_csv} for batch_id={batch_id}"
        )

    return samples


MANIFEST_SAMPLES = load_manifest(MANIFEST_CSV, TARGET_BATCH_ID)
SAMPLES = sorted(MANIFEST_SAMPLES.keys())

# Derive ROOT from fastq_local_dir (parent of ".../fastq")
_any_sample = next(iter(MANIFEST_SAMPLES.values()))
FASTQ_DIR = _any_sample["fastq_dir"]
if not FASTQ_DIR:
    raise ValueError("fastq_local_dir is empty in manifest for selected batch.")

ROOT = os.path.dirname(FASTQ_DIR)

# =============================================================================
# Paths and constants
# =============================================================================

# Where this workflow (Snakefile + config dir) lives
WORKFLOW_DIR = workflow.basedir

# I/O layout relative to ROOT (per-batch scratch area)
MERGED_DIR       = f"{ROOT}/fastq_merged"
FASTQC_DIR       = f"{ROOT}/fastqc_raw"
FASTQC_LOGS_DIR  = f"{FASTQC_DIR}/logs"
FASTP_BASE       = f"{ROOT}/fastp_outs"
FASTP_TRIM_DIR   = f"{FASTP_BASE}/trimmed_fqs"
FASTP_REPORT_DIR = f"{FASTP_BASE}/reports"
FASTP_LOG_DIR    = f"{FASTP_BASE}/logs"
FQ2BAM_BASE      = f"{ROOT}/fq2bam_gpu_outs"
MULTIQC_OUTDIR   = f"{ROOT}/multiqc_gpu_out"

# Container images (SIFs)
CONTAINER_BASE = "/global/project/hpcg6049/somatic_pipeline/containers/NXF_SINGULARITY_CACHEDIR"

FASTQC_SIF      = f"{CONTAINER_BASE}/fastqc-0.12.1--hdfd78af_0.sif"
FASTP_SIF       = f"{CONTAINER_BASE}/fastp-1.0.1--heae3180_0.sif"
SAMTOOLS_SIF    = f"{CONTAINER_BASE}/samtools-1.22.1--h96c455f_0.sif"
GATK_SIF        = f"{CONTAINER_BASE}/gatk4-4.6.2.0--py310hdfd78af_1.sif"
MULTIQC_SIF     = f"{CONTAINER_BASE}/multiqc-1.32--pyhdfd78af_0.sif"
PARABRICKS_SIF  = f"{CONTAINER_BASE}/parabrick_4.5.1.sif"

# References
REF_BASE = "/global/project/hpcg6049/somatic_pipeline/data/references"

REF_FASTA         = f"{REF_BASE}/GRCh38.d1.vd1.fa"
KNOWN_SITES_MILLS = f"{REF_BASE}/Mills.d1vd1.ready.vcf.gz"
KNOWN_SITES_1KG   = f"{REF_BASE}/1000G.indels.d1vd1.ready.vcf.gz"
KNOWN_SITES_DBSNP = f"{REF_BASE}/dbSNP.GCF40.d1vd1.ready.vcf.gz"

# MultiQC assets in the workflow repo
BASE_MULTIQC_CONFIG_TEMPLATE = f"{WORKFLOW_DIR}/config/multiqc_config.base.yaml"
LOGO_PATH = f"{WORKFLOW_DIR}/config/mohcc_logo_tiny.jpg"
CSS_PATH  = f"{WORKFLOW_DIR}/config/mohcc_multiqc.css"

# Batch-specific MultiQC config and report paths
MULTIQC_BATCH_CONFIG = f"{MULTIQC_OUTDIR}/multiqc_{TARGET_BATCH_ID}.yaml"
MULTIQC_REPORT       = f"{MULTIQC_OUTDIR}/{TARGET_BATCH_ID}_multiqc_report.html"

READS = ["R1", "R2"]

# =============================================================================
# Helper functions
# =============================================================================

def r1_lanes_for_sample(wc):
    """
    Collect all R1 FASTQs for a given sample using fastq_dir from the manifest.
    Pattern:
      <fastq_dir>/{sample}_S*_L*_R1_001.fastq.gz
    """
    info = MANIFEST_SAMPLES[wc.sample]
    fq_dir = info["fastq_dir"]
    pattern = os.path.join(fq_dir, f"{wc.sample}_S*_L*_R1_001.fastq.gz")
    files = sorted(glob.glob(pattern))
    if not files:
        raise ValueError(f"No R1 FASTQs found for sample {wc.sample} with pattern {pattern}")
    return files


def r2_lanes_for_sample(wc):
    """
    Collect all R2 FASTQs for a given sample using fastq_dir from the manifest.
    Pattern:
      <fastq_dir>/{sample}_S*_L*_R2_001.fastq.gz
    """
    info = MANIFEST_SAMPLES[wc.sample]
    fq_dir = info["fastq_dir"]
    pattern = os.path.join(fq_dir, f"{wc.sample}_S*_L*_R2_001.fastq.gz")
    files = sorted(glob.glob(pattern))
    if not files:
        raise ValueError(f"No R2 FASTQs found for sample {wc.sample} with pattern {pattern}")
    return files


# -------------------------------------------------------------------------
# Helper: write a batch-specific MultiQC config YAML
# -------------------------------------------------------------------------

def write_multiqc_config(out_path):
    """
    Create a batch-specific MultiQC config based on a base template
    and manifest metadata.
    """
    # Use the first sample as representative for batch-level fields
    info0 = next(iter(MANIFEST_SAMPLES.values()))

    project_id = info0.get("project_id") or "UNKNOWN"
    batch_id   = info0.get("batch_id") or TARGET_BATCH_ID
    flowcell   = info0.get("flowcell_id") or "UNKNOWN"
    run_date   = info0.get("run_date") or "UNKNOWN"
    report_date = datetime.date.today().isoformat()

    # Load the base config (if missing, raise a clear error)
    if not os.path.exists(BASE_MULTIQC_CONFIG_TEMPLATE):
        raise FileNotFoundError(
            f"Base MultiQC config not found at {BASE_MULTIQC_CONFIG_TEMPLATE}"
        )

    with open(BASE_MULTIQC_CONFIG_TEMPLATE) as f:
        cfg = yaml.safe_load(f) or {}

    # Override / inject the dynamic bits
    cfg.setdefault("title", "MOHCC-WGTS Quality Control Report")
    cfg["subtitle"] = f"Batch ID: {batch_id}"

    cfg["report_header_info"] = [
        {"Project": project_id},
        {"Batch ID": batch_id},
        {"Flowcell ID": flowcell},
        {"Run Date": run_date},
        {"Report Date": report_date},
    ]

    cfg["custom_logo"] = LOGO_PATH
    cfg.setdefault("custom_logo_title", "Marathon of Hope Cancer Centres Network")
    cfg["custom_css_files"] = [CSS_PATH]

    # Ensure output dir exists
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    with open(out_path, "w") as out_f:
        yaml.safe_dump(cfg, out_f, sort_keys=False)


# =============================================================================
# rule all
# =============================================================================

rule all:
    input:
        # merged raw FASTQs (temp, but still part of DAG)
        expand(f"{MERGED_DIR}/{{sample}}_R1.fastq.gz", sample=SAMPLES),
        expand(f"{MERGED_DIR}/{{sample}}_R2.fastq.gz", sample=SAMPLES),

        # FastQC on merged raw reads
        expand(f"{FASTQC_DIR}/{{sample}}_{{read}}_fastqc.html",
               sample=SAMPLES, read=READS),
        expand(f"{FASTQC_DIR}/{{sample}}_{{read}}_fastqc.zip",
               sample=SAMPLES, read=READS),

        # fastp-trimmed FASTQs
        expand(f"{FASTP_TRIM_DIR}/{{sample}}.trimmed_1.fastq.gz", sample=SAMPLES),
        expand(f"{FASTP_TRIM_DIR}/{{sample}}.trimmed_2.fastq.gz", sample=SAMPLES),

        # fq2bam (Parabricks): final analysis-ready BAM + index
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.bai", sample=SAMPLES),

        # QC metrics
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.dedup_metrics.txt", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.recal_table.txt", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.flagstat", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.wgs_metrics.txt", sample=SAMPLES),

        # MultiQC report
        MULTIQC_REPORT

# =============================================================================
# Pre-processing (merge lanes, FastQC, fastp)
# =============================================================================

rule merge_lanes:
    input:
        R1 = r1_lanes_for_sample,
        R2 = r2_lanes_for_sample
    output:
        R1 = temp(f"{MERGED_DIR}/{{sample}}_R1.fastq.gz"),
        R2 = temp(f"{MERGED_DIR}/{{sample}}_R2.fastq.gz")
    threads: 2
    resources:
        mem_mb = 4000
    message:
        "Merging lanes for sample {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail

        mkdir -p "{MERGED_DIR}"

        # R1
        cat {input.R1} > {output.R1}

        # R2
        cat {input.R2} > {output.R2}
        """


rule fastqc_raw:
    input:
        fq = f"{MERGED_DIR}/{{sample}}_{{read}}.fastq.gz"
    output:
        html = f"{FASTQC_DIR}/{{sample}}_{{read}}_fastqc.html",
        zip  = f"{FASTQC_DIR}/{{sample}}_{{read}}_fastqc.zip"
    log:
        f"{FASTQC_LOGS_DIR}/{{sample}}_{{read}}.fastqc.log"
    threads: 16
    resources:
        mem_mb = 8000,
        runtime = 480
    params:
        sif    = FASTQC_SIF,
        outdir = FASTQC_DIR
    message:
        "Running FastQC on {input.fq}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "{params.outdir}"
        mkdir -p "{FASTQC_LOGS_DIR}"

        apptainer exec --bind /global/project,/global/scratch {params.sif} \
          fastqc \
            --threads {threads} \
            --outdir {params.outdir} \
            --nogroup \
            {input.fq} \
          > {log} 2>&1
        """


rule fastp_sample:
    input:
        r1 = lambda w: f"{MERGED_DIR}/{w.sample}_R1.fastq.gz",
        r2 = lambda w: f"{MERGED_DIR}/{w.sample}_R2.fastq.gz"
    output:
        trimmed_r1 = temp(f"{FASTP_TRIM_DIR}/{{sample}}.trimmed_1.fastq.gz"),
        trimmed_r2 = temp(f"{FASTP_TRIM_DIR}/{{sample}}.trimmed_2.fastq.gz"),
        json       = f"{FASTP_REPORT_DIR}/{{sample}}.fastp.json",
        html       = f"{FASTP_REPORT_DIR}/{{sample}}.fastp.html"
    log:
        f"{FASTP_LOG_DIR}/{{sample}}.fastp.log"
    params:
        sif = FASTP_SIF
    threads: 16
    resources:
        mem_mb = 16000
    message:
        "Running fastp on merged FASTQs for sample {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "{FASTP_TRIM_DIR}"
        mkdir -p "{FASTP_REPORT_DIR}"
        mkdir -p "{FASTP_LOG_DIR}"

        apptainer exec --bind /global/project,/global/scratch {params.sif} \
          fastp \
            --in1 {input.r1} \
            --in2 {input.r2} \
            --out1 {output.trimmed_r1} \
            --out2 {output.trimmed_r2} \
            --json {output.json} \
            --html {output.html} \
            --thread {threads} \
          > {log} 2>&1
        """

# =============================================================================
# fq2bam via Parabricks (GPU)
# =============================================================================

rule fq2bam_gpu:
    """
    GPU-accelerated fq2bam using NVIDIA Parabricks pbrun fq2bam.
    Produces analysis-ready BAM + BQSR table + duplicate metrics.
    """
    input:
        r1 = lambda w: f"{FASTP_TRIM_DIR}/{w.sample}.trimmed_1.fastq.gz",
        r2 = lambda w: f"{FASTP_TRIM_DIR}/{w.sample}.trimmed_2.fastq.gz"
    output:
        final_bam     = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam",
        final_bai     = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.bai",
        recal_table   = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.recal_table.txt",
        dedup_metrics = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.dedup_metrics.txt"
    log:
        f"{FQ2BAM_BASE}/{{sample}}/logs/fq2bam_gpu.log"
    params:
        sif   = PARABRICKS_SIF,
        ref   = REF_FASTA,
        mills = KNOWN_SITES_MILLS,
        kg    = KNOWN_SITES_1KG,
        dbsnp = KNOWN_SITES_DBSNP
    threads: 32
    resources:
        mem_mb  = 184320, 
        runtime = 350
    message:
        "Running Parabricks fq2bam on GPU for {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "{FQ2BAM_BASE}/{wildcards.sample}"
        mkdir -p "$(dirname {log})"

        apptainer exec \
          --nv \
          --bind /global/project,/global/scratch \
          {params.sif} \
          pbrun fq2bam \
            --ref {params.ref} \
            --in-fq {input.r1} {input.r2} \
            --read-group-sm {wildcards.sample} \
            --read-group-lb {wildcards.sample} \
            --read-group-pl ILLUMINA \
            --read-group-id-prefix {wildcards.sample} \
            --bwa-options "-Y -K 10000000" \
            --knownSites {params.mills} \
            --knownSites {params.dbsnp} \
            --knownSites {params.kg} \
            --out-recal-file {output.recal_table} \
            --out-duplicate-metrics {output.dedup_metrics} \
            --out-qc-metrics-dir {FQ2BAM_BASE}/{wildcards.sample}/qc_metrics \
            --out-bam {output.final_bam} \
          > {log} 2>&1
        """

# =============================================================================
# QC metrics: samtools flagstat + Parabricks bammetrics
# =============================================================================

rule samtools_flagstat:
    input:
        bam = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam",
        bai = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.bai"
    output:
        txt = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.flagstat"
    log:
        f"{FQ2BAM_BASE}/{{sample}}/logs/flagstat.log"
    params:
        sif = SAMTOOLS_SIF
    threads: 2
    resources:
        mem_mb  = 8000,
        runtime = 480
    message:
        "Running samtools flagstat on analysis-ready BAM for {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "$(dirname {log})"

        apptainer exec --bind /global/project,/global/scratch {params.sif} \
          samtools flagstat {input.bam} \
          > {output.txt} 2> {log}
        """


rule collect_wgs_metrics_gpu:
    """
    GPU-accelerated WGS metrics using Parabricks pbrun bammetrics.
    Output stays in {sample}.wgs_metrics.txt for MultiQC compatibility.
    """
    input:
        bam = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam",
        bai = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.bai"
    output:
        metrics = f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.wgs_metrics.txt"
    log:
        f"{FQ2BAM_BASE}/{{sample}}/logs/wgs_metrics.log"
    params:
        sif = PARABRICKS_SIF,
        ref = REF_FASTA
    threads: 32
    resources:
        mem_mb  = 128000,
        runtime = 120
    message:
        "Collecting WGS metrics with Parabricks bammetrics for {wildcards.sample}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "$(dirname {log})"

        apptainer exec \
          --nv \
          --bind /global/project,/global/scratch \
          {params.sif} \
          pbrun bammetrics \
            --ref {params.ref} \
            --bam {input.bam} \
            --out-metrics-file {output.metrics} \
          > {log} 2>&1
        """

# =============================================================================
# MultiQC: config + report
# =============================================================================

rule multiqc_config:
    output:
        MULTIQC_BATCH_CONFIG
    run:
        write_multiqc_config(output[0])


rule multiqc:
    input:
        expand(f"{FASTP_REPORT_DIR}/{{sample}}.fastp.json", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.analysis_ready.bam.flagstat", sample=SAMPLES),
        expand(f"{FQ2BAM_BASE}/{{sample}}/{{sample}}.wgs_metrics.txt", sample=SAMPLES),
        MULTIQC_BATCH_CONFIG
    output:
        html = MULTIQC_REPORT
    log:
        f"{MULTIQC_OUTDIR}/multiqc.log"
    params:
        sif         = MULTIQC_SIF,
        fastp_dir   = FASTP_BASE,
        align_dir   = FQ2BAM_BASE,
        outdir      = MULTIQC_OUTDIR,
        batch_name  = TARGET_BATCH_ID,
        config      = MULTIQC_BATCH_CONFIG,
        report_name = f"{TARGET_BATCH_ID}_multiqc_report",
        data_dir    = f"{MULTIQC_OUTDIR}/{TARGET_BATCH_ID}_multiqc_report_data"
    threads: 2
    resources:
        mem_mb  = 16000,
        runtime = 60
    message:
        "Running MultiQC for batch {params.batch_name}"
    shell:
        r"""
        set -euo pipefail
        module --force purge
        module load StdEnv/2023
        module load apptainer

        mkdir -p "{params.outdir}"
        mkdir -p "$(dirname {log})"

        # Clean any previous report so MultiQC doesn't create *_1.html
        rm -f  "{output.html}"
        rm -rf "{params.data_dir}"

        apptainer exec \
          --bind /global/project,/global/scratch \
          {params.sif} \
          multiqc \
            -f \
            -n "{params.report_name}" \
            -o "{params.outdir}" \
            -c "{params.config}" \
            "{params.fastp_dir}" \
            "{params.align_dir}" \
          >> {log} 2>&1
        """